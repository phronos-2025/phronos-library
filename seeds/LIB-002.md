# LIB-002: Digital Validity

## Question

Under what conditions can chat data serve as valid proxies for cognitive states?

## Enables

**Instruments:** All downstream instruments
**Hypotheses:** H2.1, H2.2, H2.3, H2.4, H2.5, H2.6
**Methods:** MTH-001, MTH-002, MTH-003

## Must Establish

- Chat data has ecological validity for cognitive patterns
- Anonymous chat interactions elicit authentic self-disclosure
- Cognitive phenotype measurements show acceptable test-retest reliability (r > 0.7)
- Cognitive phenotypes generalize across AI platforms
- Selection effects are systematic and can be characterized
- Task-based assessments converge with naturalistic measures

## Scope

**In:**
- Ecological validity of chat-derived measures
- Test-retest reliability
- Cross-platform generalization
- Selection bias and power user effects
- Convergent validity with established measures
- Task-based vs. naturalistic measurement

**Out:**
- What specific constructs can be measured → LIB-001
- How measurement changes the measured → LIB-003, LIB-008
- Ethical presentation of results → LIB-008

## Key Sources

- Insel 2017 — Digital phenotyping paradigm → establishes H2.1 framework
- Torous 2016 — Mobile sensing methodology → establishes measurement approach
- Fatima 2021 — Embedding-based prediction validity → establishes H1.7 caveats
- WildChat / DSP-001 — Selection effects demonstration → establishes H2.5
- Charlesworth et al. 2021 — Word embedding bias methodology → establishes embedding validity
- Pedersen et al. 2023 — Game-based cognitive assessment validation → establishes H2.6 approach

## Current Gaps

- Test-retest reliability for chat-derived measures — critical for H2.3
- Cross-platform generalization study — affects H2.4
- Task-naturalistic convergence — affects H2.6
- Selection effect adjustment effectiveness — affects H2.5

## Confidence

**Variable by hypothesis.** H2.1-H2.2 have moderate support from digital phenotyping literature. H2.3-H2.4 are open empirical questions critical to the research program. H2.5 has high confidence that effects exist, moderate that adjustment works.

## Notes

This article is foundational—it establishes whether chat-based measurement is valid at all. Without H2.3 (reliability) and H2.6 (convergent validity), downstream instruments cannot claim validity.

Core hypotheses:
- **H2.1:** Moderate — Digital phenotyping literature supports ecological validity concept
- **H2.2:** Moderate — Self-disclosure research suggests anonymity effects
- **H2.3:** Gap — Critical empirical question; no direct studies
- **H2.4:** Gap — Requires cross-platform study
- **H2.5:** High (existence) / Moderate (adjustment) — Selection effects documented
- **H2.6:** Gap — Task-naturalistic convergence not directly studied

## Dependencies

- Requires: LIB-001 (what to measure)
- Enables: All downstream discovery and application work
