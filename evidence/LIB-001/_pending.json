[
  {
    "key": "PonizovskiyEtAl2020",
    "title": "Development and Validation of the Personal Values Dictionary: A Theory–Driven Tool for Investigating References to Basic Human Values in Text",
    "authors": "Ponizovskiy et al.",
    "year": "2020",
    "publication": "European Journal of Personality",
    "doi": "10.1002/per.2294",
    "url": "https://doi.org/10.1002/per.2294",
    "abstract": "Estimating psychological constructs from natural language has the potential to expand the reach and applicability of personality science. Research on the Big Five has produced methods to reliably assess personality traits from text, but the development of comparable tools for personal values is still in the early stages. Based on the Schwartz theory of basic human values, we developed a dictionary for the automatic assessment of references to personal values in text. To refine and validate the dictionary, we used Facebook updates, blog posts, essays, and book chapters authored by over 180 000 individuals. The results show high reliability for the dictionary and a pattern of correlations between the value types in line with the circumplex structure. We found small to moderate (rs = .1?.4) but consistent correlations between dictionary scores and self?reported scores for 7 out of 10 values. Correlations between the dictionary scores and age, gender, and political orientation of the author and scores for other established dictionaries mostly followed theoretical predictions. The Personal Values Dictionary can be used to assess references to value orientations in textual data, such as tweets, blog posts, or status updates, and will stimulate further research in methods to assess human basic values from text.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "JordanEtAl2019",
    "title": "Examining long-term trends in politics and culture through language of political leaders and cultural institutions",
    "authors": "Jordan, Sterling, Pennebaker & Boyd",
    "year": "2019",
    "publication": "Proceedings of the National Academy of Sciences",
    "doi": "10.1073/pnas.1811987116",
    "url": "https://pnas.org/doi/full/10.1073/pnas.1811987116",
    "abstract": "Significance\n            Donald Trump and a small group of emerging leaders around the world have been labeled as outliers in the ways that they think and communicate with others. Are they really anomalies, or do they fit into larger political trends? This study adds to existing scholarship by analyzing two important psychological dimensions, analytic thinking and confidence, in 12 large corpora of political texts representing political leaders of various levels in both the United States and other countries as well as 4 corpora of cultural texts. Rather than being anomalous, linguistic analyses find that, over the last century, there have been consistent declines in analytic thinking and rises in confidence in the ways that political leaders communicate with the public.\n          , \n            From many perspectives, the election of Donald Trump was seen as a departure from long-standing political norms. An analysis of Trump’s word use in the presidential debates and speeches indicated that he was exceptionally informal but at the same time, spoke with a sense of certainty. Indeed, he is lower in analytic thinking and higher in confidence than almost any previous American president. Closer analyses of linguistic trends of presidential language indicate that Trump’s language is consistent with long-term linear trends, demonstrating that he is not as much an outlier as he initially seems. Across multiple corpora from the American presidents, non-US leaders, and legislative bodies spanning decades, there has been a general decline in analytic thinking and a rise in confidence in most political contexts, with the largest and most consistent changes found in the American presidency. The results suggest that certain aspects of the language style of Donald Trump and other recent leaders reflect long-evolving political trends. Implications of the changing nature of popular elections and the role of media are discussed.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "WilsonMihalcea2019",
    "title": "Predicting Human Activities from User-Generated Content",
    "authors": "Wilson & Mihalcea",
    "year": "2019",
    "publication": "",
    "doi": "10.18653/v1/P19-1245",
    "url": "https://aclanthology.org/P19-1245/",
    "abstract": "The activities we do are linked to our interests, personality, political preferences, and decisions we make about the future. In this paper, we explore the task of predicting human activities from user-generated content. We collect a dataset containing instances of social media users writing about a range of everyday activities. We then use a state-of-the-art sentence embedding framework tailored to recognize the semantics of human activities and perform an automatic clustering of these activities. We train a neural network model to make predictions about which clusters contain activities that were performed by a given user based on the text of their previous posts and self-description. Additionally, we explore the degree to which incorporating inferred user traits into our model helps with this prediction task.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "SorensenEtAl2024",
    "title": "Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties",
    "authors": "Sorensen et al.",
    "year": "2024",
    "publication": "Proceedings of the AAAI Conference on Artificial Intelligence",
    "doi": "10.1609/aaai.v38i18.29970",
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/29970",
    "abstract": "Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "RyuEtAl2023",
    "title": "A natural language processing approach reveals first-person pronoun usage and non-fluency as markers of therapeutic alliance in psychotherapy",
    "authors": "Ryu et al.",
    "year": "2023",
    "publication": "iScience",
    "doi": "10.1016/j.isci.2023.106860",
    "url": "https://www.cell.com/iscience/abstract/S2589-0042(23)00937-9",
    "abstract": "<h2>Summary</h2><p>It remains elusive what language markers derived from psychotherapy sessions are indicative of therapeutic alliance, limiting our capacity to assess and provide feedback on the trusting quality of the patient-clinician relationship. To address this critical knowledge gap, we leveraged feature extraction methods from natural language processing (NLP), a subfield of artificial intelligence, to quantify pronoun and non-fluency language markers that are relevant for communicative and emotional aspects of therapeutic relationships. From twenty-eight transcripts of non-manualized psychotherapy sessions recorded in outpatient clinics, we identified therapists' first-person pronoun usage frequency and patients' speech transition marking relaxed interaction style as potential metrics of alliance. Behavioral data from patients who played an economic game that measures social exchange (i.e. trust game) suggested that therapists' first-person pronoun usage may influence alliance ratings through their diminished trusting behavior toward therapists. Together, this work supports that communicative language features in patient-therapist dialogues could be markers of alliance.</p>",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "BiggiogeraEtAl2021",
    "title": "BERT meets LIWC: Exploring State-of-the-Art Language Models for Predicting Communication Behavior in Couples’ Conflict Interactions",
    "authors": "Biggiogera et al.",
    "year": "2021",
    "publication": "",
    "doi": "10.1145/3461615.3485423",
    "url": "https://dl.acm.org/doi/10.1145/3461615.3485423",
    "abstract": "",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "IrelandPennebaker2010",
    "title": "Language style matching in writing: Synchrony in essays, correspondence, and poetry.",
    "authors": "Ireland & Pennebaker",
    "year": "2010",
    "publication": "Journal of Personality and Social Psychology",
    "doi": "10.1037/a0020386",
    "url": "https://doi.apa.org/doi/10.1037/a0020386",
    "abstract": "Each relationship has its own personality. Almost immediately after a social interaction begins, verbal and nonverbal behaviors become synchronized. Even in asocial contexts, individuals tend to produce utterances that match the grammatical structure of sentences they have recently heard or read. Three projects explore language style matching (LSM) in everyday writing tasks and professional writing. LSM is the relative use of 9 function word categories (e.g., articles, personal pronouns) between any 2 texts. In the first project, 2 samples totaling 1,744 college students answered 4 essay questions written in very different styles. Students automatically matched the language style of the target questions. Overall, the LSM metric was internally consistent and reliable across writing tasks. Women, participants of higher socioeconomic status, and students who earned higher test grades matched with targets more than others did. In the second project, 74 participants completed cliffhanger excerpts from popular fiction. Judges’ ratings of excerpt–response similarity were related to content matching but not function word matching, as indexed by LSM. Further, participants were not able to intentionally increase style or content matching. In the final project, an archival study tracked the professional writing and personal correspondence of 3 pairs of famous writers across their relationships. Language matching in poetry and letters reflected fluctuations in the relationships of 3 couples: Sigmund Freud and Carl Jung, Elizabeth Barrett and Robert Browning, and Sylvia Plath and Ted Hughes. Implications for using LSM as an implicit marker of social engagement and influence are discussed.",
    "status": "cite",
    "role": {
      "establishes": [
        "H1.1: Language style matching",
        "Linguistic synchrony in relationships"
      ],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "IrelandEtAl2011",
    "title": "Language Style Matching Predicts Relationship Initiation and Stability",
    "authors": "Ireland et al.",
    "year": "2011",
    "publication": "Psychological Science",
    "doi": "10.1177/0956797610392928",
    "url": "https://journals.sagepub.com/doi/10.1177/0956797610392928",
    "abstract": "Previous relationship research has largely ignored the importance of similarity in how people talk with one another. Using natural language samples, we investigated whether similarity in dyads’ use of function words, called language style matching (LSM), predicts outcomes for romantic relationships. In Study 1, greater LSM in transcripts of 40 speed dates predicted increased likelihood of mutual romantic interest (odds ratio = 3.05). Overall, 33.3% of pairs with LSM above the median mutually desired future contact, compared with 9.1% of pairs with LSM at or below the median. In Study 2, LSM in 86 couples’ instant messages positively predicted relationship stability at a 3-month follow-up (odds ratio = 1.95). Specifically, 76.7% of couples with LSM greater than the median were still dating at the follow-up, compared with 53.5% of couples with LSM at or below the median. LSM appears to reflect implicit interpersonal processes central to romantic relationships.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "Boyd",
    "title": "The Development and Psychometric Properties of LIWC-22",
    "authors": "Boyd",
    "year": "",
    "publication": "",
    "doi": "",
    "url": "",
    "abstract": "",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "KoutsoumpisEtAl2022",
    "title": "The kernel of truth in text-based personality assessment: A meta-analysis of the relations between the Big Five and the Linguistic Inquiry and Word Count (LIWC)",
    "authors": "Koutsoumpis et al.",
    "year": "2022",
    "publication": "Psychological Bulletin",
    "doi": "10.1037/bul0000381",
    "url": "",
    "abstract": "The Linguistic Inquiry and Word Count (LIWC) is a popular closed-vocabulary text analysis software program that is used to understand whether individuals’ use of linguistic categories (i.e., word categories, such as negative affect) depends on their personality traits. Here, we present the first meta-analysis of the relations between the Big Five personality traits and 52 linguistic categories of the English language. Across 31 eligible samples (n = 85,724), the results showed that (a) self-reported personality traits are significantly correlated with linguistic categories, but the effect sizes are relatively small (the strongest effect sizes between the Big Five and linguistic categories ranged from |ρ| = .08 to .14, and the 52 LIWC categories explained on average 5.1% of personality variance); (b) observer-reported personality traits are significantly correlated with linguistic categories, with the effect sizes being small-to-medium (|ρ| = .18–.39, explaining 38.5% of personality variance); (c) 20 linguistic categories (out of 260; 5 Personality Traits × 52 LIWC Categories) correlated both with self- and observer-reported personality traits (the “kernel of truth” in linguistic markers of personality); and (d) 10 study, sample, and task characteristics significantly moderated the correlations of the linguistic categories with personality traits, showing that the effect sizes were mainly stronger for longer texts and older LIWC versions, among others. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
    "status": "cite",
    "role": {
      "establishes": [
        "H1.5: LIWC-Big Five correlations",
        "Meta-analytic evidence for personality from text"
      ],
      "supports_instruments": [],
      "limitations": [
        "Small effect sizes",
        "Chat-specific validation needed"
      ]
    },
    "annotations": []
  },
  {
    "key": "Pennebaker2011",
    "title": "The secret life of pronouns",
    "authors": "Pennebaker",
    "year": "2011",
    "publication": "New Scientist",
    "doi": "10.1016/S0262-4079(11)62167-2",
    "url": "https://linkinghub.elsevier.com/retrieve/pii/S0262407911621672",
    "abstract": "",
    "status": "anchor",
    "role": {
      "establishes": [
        "H1.1: Pronoun patterns correlate with psychological states",
        "LIWC methodology for linguistic analysis",
        "Function words as psychological markers"
      ],
      "supports_instruments": [],
      "limitations": [
        "Dictionary-based approach may miss context"
      ]
    },
    "annotations": []
  },
  {
    "key": "StajnerYenikent2020",
    "title": "A Survey of Automatic Personality Detection from Texts",
    "authors": "Stajner & Yenikent",
    "year": "2020",
    "publication": "",
    "doi": "10.18653/v1/2020.coling-main.553",
    "url": "https://aclanthology.org/2020.coling-main.553/",
    "abstract": "Personality profiling has long been used in psychology to predict life outcomes. Recently, automatic detection of personality traits from written messages has gained significant attention in computational linguistics and natural language processing communities, due to its applicability in various fields. In this survey, we show the trajectory of research towards automatic personality detection from purely psychology approaches, through psycholinguistics, to the recent purely natural language processing approaches on large datasets automatically extracted from social media. We point out what has been gained and what lost during that trajectory, and show what can be realistic expectations in the field.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "SánchezEtAl2022",
    "title": "A White-Box Sociolinguistic Model for Gender Detection",
    "authors": "Sánchez, Moreno & López",
    "year": "2022",
    "publication": "Applied Sciences",
    "doi": "10.3390/app12052676",
    "url": "https://www.mdpi.com/2076-3417/12/5/2676",
    "abstract": "Within the area of Natural Language Processing, we approached the Author Proﬁling task as a text classiﬁcation problem. Based on the author’s writing style, sociodemographic information, such as the author’s gender, age, or native language can be predicted. The exponential growth of user-generated data and the development of Machine-Learning techniques have led to signiﬁcant advances in automatic gender detection. Unfortunately, gender detection models often become black-boxes in terms of interpretability. In this paper, we propose a tree-based computational model for gender detection made up of 198 features. Unlike the previous works on gender detection, we organized the features from a linguistic perspective into six categories: orthographic, morphological, lexical, syntactic, digital, and pragmatics-discursive. We implemented a Decision-Tree classiﬁer to evaluate the performance of all feature combinations, and the experiments revealed that, on average, the classiﬁcation accuracy increased up to 3.25% with the addition of feature sets. The maximum classiﬁcation accuracy was reached by a three-level model that combined lexical, syntactic, and digital features. We present the most relevant features for gender detection according to the trees generated by the classiﬁer and contextualize the signiﬁcance of the computational results with the linguistic patterns deﬁned by previous research in relation to gender.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "CorkEtAl2022",
    "title": "Collective self-understanding: A linguistic style analysis of naturally occurring text data",
    "authors": "Cork et al.",
    "year": "2022",
    "publication": "Behavior Research Methods",
    "doi": "10.3758/s13428-022-02027-8",
    "url": "https://link.springer.com/10.3758/s13428-022-02027-8",
    "abstract": "Understanding what groups stand for is integral to a diverse array of social processes, ranging from understanding political conflicts to organisational behaviour to promoting public health behaviours. Traditionally, researchers rely on self-report methods such as interviews and surveys to assess groups’ collective self-understandings. Here, we demonstrate the value of using naturally occurring online textual data to map the similarities and differences between real-world groups’ collective selfunderstandings. We use machine learning algorithms to assess similarities between 15 diverse online groups’ linguistic style, and then use multidimensional scaling to map the groups in two-dimensonal space (N=1,779,098 Reddit comments). We then use agglomerative and k-means clustering techniques to assess how the 15 groups cluster, finding there are four behaviourally distinct group types – vocational, collective action (comprising political and ethnic/religious identities), relational and stigmatised groups, with stigmatised groups having a less distinctive behavioural profile than the other group types. Study 2 is a secondary data analysis where we find strong relationships between the coordinates of each group in multidimensional space and the groups’ values. In Study 3, we demonstrate how this approach can be used to track the development of groups’ collective self-understandings over time. Using transgender Reddit data (N= 1,095,620 comments) as a proof-of-concept, we track the gradual politicisation of the transgender group over the past decade. The automaticity of this methodology renders it advantageous for monitoring multiple online groups simultaneously. This approach has implications for both governmental agencies and social researchers more generally. Future research avenues and applications are discussed.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "SmithEtAl2020",
    "title": "Detecting psychological change through mobilizing interactions and changes in extremist linguistic style",
    "authors": "Smith et al.",
    "year": "2020",
    "publication": "Computers in Human Behavior",
    "doi": "10.1016/j.chb.2020.106298",
    "url": "https://linkinghub.elsevier.com/retrieve/pii/S0747563220300534",
    "abstract": "Social media interactions are popularly implicated in psychological changes like radicalization. However, there are currently no viable methods to assess whether social media interactions actually lead to such changes. The purpose of the current research was to develop a methodological paradigm that can assess such longitudinal change in individuals’ social media posts. Using this method, we analyzed the longitudinal timelines of 110 Twitter users (40,053 tweets) who had expressed support for Daesh (also known as Islamic State, or ISIS) and we compared them to a baseline sample of twitter timelines (215,008 tweets by 109 users) to investigate the factors associated with within-person increases in conformity to the vernacular and linguistic style of tweets that supported violent extremism. We found that conformity to both extremist group vernacular and linguistic style increased over time, and with mobilizing online interactions. Thus, we show how to detect within-person changes over time in social media data and suggest why these changes occur, and in doing so, validate a methodological paradigm that can detect and predict within-person change in psychological group memberships through social media interactions.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "PuertasEtAl2021",
    "title": "Detection of Sociolinguistic Features in Digital Social Networks for the Detection of Communities",
    "authors": "Puertas et al.",
    "year": "2021",
    "publication": "Cognitive Computation",
    "doi": "10.1007/s12559-021-09818-9",
    "url": "http://link.springer.com/10.1007/s12559-021-09818-9",
    "abstract": "The emergence of digital social networks has transformed society, social groups, and institutions in terms of the communication and expression of their opinions. Determining how language variations allow the detection of communities, together with the relevance of specific vocabulary (proposed by the National Council of Accreditation of Colombia (Consejo Nacional de Acreditación - CNA) to determine the quality evaluation parameters for universities in Colombia) in digital assemblages could lead to a better understanding of their dynamics and social foundations, thus resulting in better communication policies and intervention where necessary. The approach presented in this paper intends to determine what are the semantic spaces (sociolinguistic features) shared by social groups in digital social networks. It includes five layers based on Design Science Research, which are integrated with Natural Language Processing techniques (NLP), Computational Linguistics (CL), and Artificial Intelligence (AI). The approach is validated through a case study wherein the semantic values of a series of “Twitter” institutional accounts belonging to Colombian Universities are analyzed in terms of the 12 quality factors established by CNA. In addition, the topics and the sociolect used by different actors in the university communities are also analyzed. The current approach allows determining the sociolinguistic features of social groups in digital social networks. Its application allows detecting the words or concepts to which each actor of a social group (university) gives more importance in terms of vocabulary.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "WelchEtAl2019",
    "title": "Look Who's Talking: Inferring Speaker Attributes from Personal Longitudinal Dialog",
    "authors": "Welch, Pérez-Rosas, Kummerfeld & Mihalcea",
    "year": "2019",
    "publication": "",
    "doi": "10.48550/arXiv.1904.11610",
    "url": "http://arxiv.org/abs/1904.11610",
    "abstract": "We examine a large dialog corpus obtained from the conversation history of a single individual with 104 conversation partners. The corpus consists of half a million instant messages, across several messaging platforms. We focus our analyses on seven speaker attributes, each of which partitions the set of speakers, namely: gender; relative age; family member; romantic partner; classmate; co-worker; and native to the same country. In addition to the content of the messages, we examine conversational aspects such as the time messages are sent, messaging frequency, psycholinguistic word categories, linguistic mirroring, and graph-based features reflecting how people in the corpus mention each other. We present two sets of experiments predicting each attribute using (1) short context windows; and (2) a larger set of messages. We find that using all features leads to gains of 9-14% over using message text only.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "MairesseEtAl2007",
    "title": "Using Linguistic Cues for the Automatic Recognition of Personality in Conversation and Text",
    "authors": "Mairesse, Walker, Mehl & Moore",
    "year": "2007",
    "publication": "Journal of Artificial Intelligence Research",
    "doi": "10.1613/jair.2349",
    "url": "https://jair.org/index.php/jair/article/view/10520",
    "abstract": "It is well known that utterances convey a great deal of information about the speaker in addition to their semantic content. One such type of information consists of cues to the speaker’s personality traits, the most fundamental dimension of variation between humans. Recent work explores the automatic detection of other types of pragmatic variation in text and conversation, such as emotion, deception, speaker charisma, dominance, point of view, subjectivity, opinion and sentiment. Personality aﬀects these other aspects of linguistic production, and thus personality recognition may be useful for these tasks, in addition to many other potential applications. However, to date, there is little work on the automatic recognition of personality traits. This article reports experimental results for recognition of all Big Five personality traits, in both conversation and text, utilising both self and observer ratings of personality. While other work reports classiﬁcation results, we experiment with classiﬁcation, regression and ranking models. For each model, we analyse the eﬀect of diﬀerent feature sets on accuracy. Results show that for some traits, any type of statistical model performs signiﬁcantly better than the baseline, but ranking models perform best overall. We also present an experiment suggesting that ranking models are more accurate than multi-class classiﬁers for modelling personality. In addition, recognition models trained on observed personality perform better than models trained using selfreports, and the optimal feature set depends on the personality trait. A qualitative analysis of the learned models conﬁrms previous ﬁndings linking language and personality, while revealing many new linguistic markers.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "ShankarEtAl2025",
    "title": "A Systematic Review of Natural Language Processing Techniques for Early Detection of Cognitive Impairment",
    "authors": "Shankar, Bundele & Mukhopadhyay",
    "year": "2025",
    "publication": "Mayo Clinic Proceedings: Digital Health",
    "doi": "10.1016/j.mcpdig.2025.100205",
    "url": "https://www.sciencedirect.com/science/article/pii/S2949761225000124",
    "abstract": "Objective\nTo systematically evaluate the effectiveness and methodologic approaches of natural language processing (NLP) techniques for early detection of cognitive decline through speech and language analysis.\nMethods\nWe conducted a comprehensive search of 8 databases from inception through August 31, 2024, following Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. Studies were included if they used NLP techniques to analyze speech or language data for detecting cognitive impairment and reported diagnostic accuracy metrics. Two independent reviewers (R.S. and A.B.) screened articles and extracted data on study characteristics, NLP methods, and outcomes.\nResults\nOf 23,562 records identified, 51 studies met inclusion criteria, involving 17,340 participants (mean age, 72.4 years). Combined linguistic and acoustic approaches achieved the highest diagnostic accuracy (average 87%; area under the curve [AUC], 0.89) compared with linguistic-only (83%; AUC, 0.85) or acoustic-only approaches (80%; AUC, 0.82). Lexical diversity, syntactic complexity, and semantic coherence were consistently strong predictors across cognitive conditions. Picture description tasks were most common (n=21), followed by spontaneous speech (n=15) and story recall (n=8). Crosslinguistic applicability was found across 8 languages, although language-specific adaptations were necessary. Longitudinal studies (n=9) reported potential for early detection but were limited by smaller sample sizes (average n=159) compared with cross-sectional studies (n=42; average n=274).\nConclusion\nNatural language processing techniques show promising diagnostic accuracy for detecting cognitive impairment across multiple languages and clinical contexts. Although combined linguistic-acoustic approaches appear most effective, methodologic heterogeneity and small sample sizes in existing studies suggest the need for larger, standardized investigations to establish clinical utility.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "CannizzaroCoelho2013",
    "title": "Analysis of Narrative Discourse Structure as an Ecologically Relevant Measure of Executive Function in Adults",
    "authors": "Cannizzaro & Coelho",
    "year": "2013",
    "publication": "Journal of Psycholinguistic Research",
    "doi": "10.1007/s10936-012-9231-5",
    "url": "http://link.springer.com/10.1007/s10936-012-9231-5",
    "abstract": "This study examined the narrative discourse production and executive function (EF) abilities of 46 neuro-typical adults (18–98 years old). Two questions were addressed: Is the analysis of narrative structure sensitive to changes associated with aging? & What is the relationship between measures of narrative structure and EF? Narratives were elicited under two conditions and narrative structure was analyzed for the presence of organizing story grammar elements. Narrative structure was signiﬁcantly correlated with age as well as linguistic and non-linguistic measures of EF. Factor analysis of story structure and EF variables yielded two factors reﬂecting constructs of output-ﬂuidity and organizational-efﬁciency. These data suggest that narrative structure and EF represent aspects of goal-directed knowledge that are not bound by a traditional linguistic and non-linguistic division. Thus, narrative structure may represent a global and ecologically valid measure of goal-directed executive function knowledge that is also sensitive to changes associated with typical aging.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "SilviaEtAl2008",
    "title": "Assessing creativity with divergent thinking tasks: Exploring the reliability and validity of new subjective scoring methods.",
    "authors": "Silvia et al.",
    "year": "2008",
    "publication": "Psychology of Aesthetics, Creativity, and the Arts",
    "doi": "10.1037/1931-3896.2.2.68",
    "url": "https://doi.apa.org/doi/10.1037/1931-3896.2.2.68",
    "abstract": "Divergent thinking is central to the study of individual differences in creativity, but the traditional scoring systems (assigning points for infrequent responses and summing the points) face well-known problems. After critically reviewing past scoring methods, this article describes a new approach to assessing divergent thinking and appraises its reliability and validity. In our new Top 2 scoring method, participants complete a divergent thinking task and then circle the 2 responses that they think are their most creative responses. Raters then evaluate the responses on a 5-point scale. Regarding reliability, a generalizability analysis showed that subjective ratings of unusual-uses tasks and instances tasks yield dependable scores with only 2 or 3 raters. Regarding validity, a latent-variable study (n 226) predicted divergent thinking from the Big Five factors and their higher-order traits (Plasticity and Stability). Over half of the variance in divergent thinking could be explained by dimensions of personality. The article presents instructions for measuring divergent thinking with the new method.",
    "status": "cite",
    "role": {
      "establishes": [
        "Subjective creativity scoring methodology",
        "Top-2 scoring method for divergent thinking"
      ],
      "supports_instruments": [
        "INS-001.1"
      ],
      "limitations": [
        "Requires rater training for reliability"
      ]
    },
    "annotations": []
  },
  {
    "key": "ArribaPérezEtAl2023",
    "title": "Automatic detection of cognitive impairment in elderly people using an entertainment chatbot with Natural Language Processing capabilities",
    "authors": "Arriba-Pérez, García-Méndez, González-Castaño & Costa-Montenegro",
    "year": "2023",
    "publication": "Journal of Ambient Intelligence and Humanized Computing",
    "doi": "10.1007/s12652-022-03849-2",
    "url": "https://link.springer.com/10.1007/s12652-022-03849-2",
    "abstract": "Previous researchers have proposed intelligent systems for therapeutic monitoring of cognitive impairments. However, most existing practical approaches for this purpose are based on manual tests. This raises issues such as excessive caretaking effort and the white-coat effect. To avoid these issues, we present an intelligent conversational system for entertaining elderly people with news of their interest that monitors cognitive impairment transparently. Automatic chatbot dialogue stages allow assessing content description skills and detecting cognitive impairment with Machine Learning algorithms. We create these dialogue flows automatically from updated news items using Natural Language Generation techniques. The system also infers the gold standard of the answers to the questions, so it can assess cognitive capabilities automatically by comparing these answers with the user responses. It employs a similarity metric with values in [0, 1], in increasing level of similarity. To evaluate the performance and usability of our approach, we have conducted field tests with a test group of 30 elderly people in the earliest stages of dementia, under the supervision of gerontologists. In the experiments, we have analysed the effect of stress and concentration in these users. Those without cognitive impairment performed up to five times better. In particular, the similarity metric varied between 0.03, for stressed and unfocused participants, and 0.36, for relaxed and focused users. Finally, we developed a Machine Learning algorithm based on textual analysis features for automatic cognitive impairment detection, which attained accuracy, F-measure and recall levels above 80%. We have thus validated the automatic approach to detect cognitive impairment in elderly people based on entertainment content. The results suggest that the solution has strong potential for long-term user-friendly therapeutic monitoring of elderly people.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "BudanitskyHirst2006",
    "title": "Evaluating WordNet-based Measures of Lexical Semantic Relatedness",
    "authors": "Budanitsky & Hirst",
    "year": "2006",
    "publication": "Computational Linguistics",
    "doi": "10.1162/coli.2006.32.1.13",
    "url": "https://aclanthology.org/J06-1003/",
    "abstract": "",
    "status": "cite",
    "role": {
      "establishes": [
        "WordNet-based semantic similarity measures",
        "Evaluation framework for lexical relatedness"
      ],
      "supports_instruments": [
        "INS-001.1",
        "INS-001.2"
      ],
      "limitations": [
        "Knowledge-based vs distributional approaches differ"
      ]
    },
    "annotations": []
  },
  {
    "key": "AugusteEtAl2017",
    "title": "Evaluation of word embeddings against cognitive processes: primed reaction times in lexical decision and naming tasks",
    "authors": "Auguste, Rey & Favre",
    "year": "2017",
    "publication": "",
    "doi": "10.18653/v1/W17-5304",
    "url": "http://aclweb.org/anthology/W17-5304",
    "abstract": "This work presents a framework for word similarity evaluation grounded on cognitive sciences experimental data. Word pair similarities are compared to reaction times of subjects in large scale lexical decision and naming tasks under semantic priming. Results show that GloVe embeddings lead to signiﬁcantly higher correlation with experimental measurements than other controlled and off-the-shelf embeddings, and that the choice of a training corpus is less important than that of the algorithm. Comparison of rankings with other datasets shows that the cognitive phenomenon covers more aspects than simply word relatedness or similarity.",
    "status": "cite",
    "role": {
      "establishes": [
        "H1.7: Embedding distances correlate with cognitive semantic distance",
        "Reaction time correlation with cosine similarity",
        "GloVe vs other embeddings comparison"
      ],
      "supports_instruments": [
        "INS-001.1",
        "INS-001.2"
      ],
      "limitations": [
        "Population-level validation; individual differences less established",
        "Different corpora yield different embedding spaces"
      ]
    },
    "annotations": [
      {
        "text": "“Intrinsic and extrinsic approaches have been proposed for word embedding evaluation. The former typically consist in collecting human judgment of word similarity on a range of word pairs, and computing the rank correlation of their averaged value with the cosine similarity between the embeddings of the words in the pair. Word analogy is also evaluated but it has been proven to be equivalent to a linear combination between cosine similarities (Levy et al., 2014).”",
        "page": 21,
        "note": ""
      },
      {
        "text": "“There is an extensive psychological literature concerning the nature of semantic representations and the influence of semantic or associative context on word processing (McNamara, 2005). In this domain, the semantic priming paradigm is one of the most popular experimental tool to study these cognitive processes. In this task, participants are presented with a prime (stimulus) word (e.g., cat) immediately followed by either a related (e.g., dog) or an unrelated (e.g., truck) target word. A speeded response is expected on the target word (e.g., a lexical-decision, i.e., is it a word or not?) and a response time is recorded. Semantic priming refers to the finding that people respond faster to target words preceded by related, relative to unrelated, primes. This behavioral index therefore provides information about the influence of a semantic context (i.e., the prime word) on the processing of the target word and is suitable for the development of theories of semantic memory.”",
        "page": 22,
        "note": ""
      },
      {
        "text": "“The Semantic Priming Project (SPP) (Hutchison et al., 2013) is one of these recently collected databases. It provides response times from 768 participants in speeded naming (NT) and lexical decision (LDT) tasks for 1,661 target words following related and unrelated primes. The naming task consists in reading aloud the target, while the lexical decision task consists in pressing one of two buttons to specify if the target is a valid word or not. Aside from the relatedness between the prime and the target, the item data is also available for stimulus onset asynchronies (SOA) between the prime and target items of 200 and 1,200 ms. Stimulus onset asynchrony is the time between the end of showing the first (stimulus) word, and the target word in the reaction time experiments.”",
        "page": 22,
        "note": ""
      },
      {
        "text": "“we evaluate embeddings by computing the correlation between the cosine similarity of pairs of words and reaction times (RT). Shorter RT indicate more priming effect, leading to negative correlations. In addition, non linguistic factors such as frequency are known to influence RT measurements, so it is not expected that word embeddings explain the whole variance of the experiment. The SPP data is significantly larger than word similarity datasets and consists of 6,637 word pairs.”",
        "page": 23,
        "note": ""
      },
      {
        "text": "“For the embeddings with controlled settings, we used two algorithms: W2V Skip-grams and GloVe. We used three different corpora to train these embeddings: Wikipedia 2013 (Wiki), Gigaword3 (GW) and OpenSubtitles 2016 (OS). We used a centered window of size 10 and generated vectors with 100 dimensions for all 6 models.”",
        "page": 23,
        "note": ""
      },
      {
        "text": "“It is not clear what cognitive processes lead to mental representations of words in the brain, and it is not clear how these processes relate to linguistic theories. However, reaction time in the context of semantic priming seems to be a good proxy for modeling word embeddings after cognitive processes. The proposed framework addresses some of the problems with word embedding evaluation exposed in (Faruqui et al., 2016)5: (2.1) subjectivity is addressed by looking at unconscious phenomena, (2.2) the lexical decision and naming tasks are very general but they are affected by other cognitive pipelines such as vision, (2.3) we provide standardized splits, and (2.5) the size of the dataset allows for significant differences between algorithms. However, we do not address the problem of low correlation with extrinsic evaluation (2.4), we do not account for frequency effects (2.6) and polysemy (2.7). These aspects are left for future work.”",
        "page": 24,
        "note": ""
      }
    ]
  },
  {
    "key": "PennEtAl2010",
    "title": "Executive function and conversational strategies in bilingual aphasia",
    "authors": "Penn, Frankel, Watermeyer & Russell",
    "year": "2010",
    "publication": "Aphasiology",
    "doi": "10.1080/02687030902958399",
    "url": "https://doi.org/10.1080/02687030902958399",
    "abstract": "Background: Deficits of executive function (EF) have been proposed as all or part of the underlying mechanisms of language impairment in at least some types of aphasia. Executive functions also play a role in the recovery process. There is evidence that bilingual persons have some executive functioning advantages compared to monolingual persons. In this paper we combine two lines of recent investigation in order to explore the relationship between executive function and conversational strategies in bilingual aphasia. Aims: The aim of this preliminary research was to compare the executive functioning profiles of bilingual individuals to those of monolingual participants with aphasia. A further aim was to examine evidence in the conversational samples of the participants in relation to the application of a range of executive skills and to link cognitive and conversational profiles using Barkley's (1997) model of executive functions. Methods & Procedures: The performance of two bilingual individuals with aphasia on a test battery of executive function tests was compared with that of eight monolingual persons (seven with aphasia and one with right hemisphere damage). The test battery included measures of behavioural inhibition, working memory, problem solving, and reconstitution. The presence or absence of executive features in the conversational samples of the participants was judged by four raters using conversational analysis methods. Outcomes & Results: Significant differences were found between the scores of the bilingual participants and those of the monolingual participants on measures of behavioural inhibition, working memory, planning and problem solving, and reconstitution. The bilingual participants' scores were mostly within normal limits and suggested well‐retained executive functions. Conversation analysis showed evidence of differential application of these executive functions to conversational management. Regardless of severity or type of aphasia, the bilingual participants showed evidence of good topic management, repair, and flexibility compared to the monolingual participants. Conclusions: The results are interpreted in relation to current issues in bilingualism. Our preliminary findings shed light on differential approaches to assessment, therapy, and choice of language for bilingual aphasia.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "MouslihEtAl2025",
    "title": "Linguistic Markers of Theory of Mind in Spontaneous Speech: A Narrative Review",
    "authors": "Mouslih, Hodgins, Palaniyappan & Titone",
    "year": "2025",
    "publication": "Behavioral Sciences",
    "doi": "10.3390/bs15081016",
    "url": "https://www.mdpi.com/2076-328X/15/8/1016",
    "abstract": "The relationship between theory of mind (ToM) or mentalizing, i.e., the cognitive ability to attribute mental states to oneself and others, and language has been widely explored across disciplines. Identifying reliable linguistic markers of ToM extractable from individuals’ speech provides a promising path for both research and clinical practice. In this narrative review, we aimed to synthesize findings from studies identified through a PSYCINFO search to provide an overview of speech-based markers associated with ToM abilities. Our results revealed six primary categories of relevant speech markers: mental state terms, general linguistic ability, embedded clauses, referring expressions, and pragmatic markers. Standardizing these markers could enhance the replicability and applicability of ToM assessments across diverse populations. We encourage future research to build on these findings to examine how mentalizing is expressed through language in varied social, cultural, and clinical contexts. Advancing this line of inquiry will deepen our understanding of the interplay between language and mentalizing and contribute to broader insights into language and cognition.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "DevineEtAl2023",
    "title": "Machine learning and deep learning systems for automated measurement of “advanced” theory of mind: Reliability and validity in children and adolescents",
    "authors": "Devine et al.",
    "year": "2023",
    "publication": "Psychological Assessment",
    "doi": "10.1037/pas0001186",
    "url": "",
    "abstract": "Understanding individual differences in theory of mind (ToM; the ability to attribute mental states to others) in middle childhood and adolescence hinges on the availability of robust and scalable measures. Open-ended response tasks yield valid indicators of ToM but are labor intensive and difficult to compare across studies. We examined the reliability and validity of new machine learning and deep learning neural network automated scoring systems for measuring ToM in children and adolescents. Two large samples of British children and adolescents aged between 7 and 13 years (Sample 1: N = 1,135, Mage = 10.22 years, SD = 1.45; Sample 2: N = 1,020, Mage = 10.36 years, SD = 1.27) completed the silent film and strange stories tasks. Teachers rated Sample 2 children’s social competence with peers. A single latent-factor explained variation in performance on both the silent film and strange stories task (in Sample 1 and 2) and test performance was sensitive to age-related differences and individual differences within each age-group. A deep learning neural network automated scoring system trained on Sample 1 exhibited interrater reliability and measurement invariance with manual ratings in Sample 2. Validity of ratings from the automated scoring system was supported by unique positive associations between ToM and teacher-rated social competence. The results demonstrate that reliable and valid measures of ToM can be obtained using the new freely available deep learning neural network automated scoring system to rate open-ended text responses. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "OlsonEtAl2021",
    "title": "Naming unrelated words predicts creativity",
    "authors": "Olson et al.",
    "year": "2021",
    "publication": "Proceedings of the National Academy of Sciences",
    "doi": "10.1073/pnas.2022340118",
    "url": "https://www.pnas.org/doi/10.1073/pnas.2022340118",
    "abstract": "Several theories posit that creative people are able to generate more divergent ideas. If this is correct, simply naming unrelated words and then measuring the semantic distance between them could serve as an objective measure of divergent thinking. To test this hypothesis, we asked 8,914 participants to name 10 words that are as different from each other as possible. A computational algorithm then estimated the average semantic distance between the words; related words (e.g., cat and dog) have shorter distances than unrelated ones (e.g., cat and thimble). We predicted that people producing greater semantic distances would also score higher on traditional creativity measures. In Study 1, we found moderate to strong correlations between semantic distance and two widely used creativity measures (the Alternative Uses Task and the Bridge-the-Associative-Gap Task). In Study 2, with participants from 98 countries, semantic distances varied only slightly by basic demographic variables. There was also a positive correlation between semantic distance and performance on a range of problems known to predict creativity. Overall, semantic distance correlated at least as strongly with established creativity measures as those measures did with each other. Naming unrelated words in what we call the Divergent Association Task can thus serve as a brief, reliable, and objective measure of divergent thinking.",
    "status": "anchor",
    "role": {
      "establishes": [
        "H1.8: DAT methodology measures divergent thinking",
        "Mean pairwise semantic distance as creativity measure",
        "DAT-AUT correlation r=0.40",
        "DAT-RAT correlation r=0.28"
      ],
      "supports_instruments": [
        "INS-001.1",
        "INS-001.2"
      ],
      "limitations": [
        "Original DAT uses 10 words; INS-001 uses 2-5",
        "Validated on maximization task vs constrained tasks"
      ]
    },
    "annotations": [
      {
        "text": "“T hink of three words that are as different from each other as possible. Choosing these words relies on generating remote associations while inhibiting common ones, according to two dominant theories of creativity (1, 2). Associative theories posit that creative people have a semantic memory structure that makes it easier to link remote elements (3–6). Executive theories focus on top-down control of attention; creative solutions arise from monitoring and inhibiting common associations (2, 7). Based on these theories, we hypothesized that the simple act of naming unrelated words may reliably measure verbal creativity.”",
        "page": 1,
        "note": ""
      },
      {
        "text": "“To address these limitations, recent efforts have moved toward using computational algorithms to score task responses (11, 17, 26, 27). Compared with manual scoring, computational methods may also clarify the theoretical grounding of the measures since the assumptions required to score the responses must be made explicit in the program code (11, 13). Researchers have successfully automated the scoring of a broad range of creativity measures that examine noun–verb pairs (28), synonyms (29), and chains of word associations (12, 30). Many of these computational methods can generate scores similar to human ratings, including on the Alternative Uses Task (11, 17, 26, 27).”",
        "page": 1,
        "note": ""
      },
      {
        "text": "“Our proposed measure, the Divergent Association Task (DAT), asks participants to generate 10 nouns that are as different from each other as possible in all meanings and uses of the words. We then compute the semantic distances between them; words that are used in similar contexts have smaller distances. The words cat and dog, for example, would be close to each other since they are often used together, whereas cat and thimble would not. We computed the semantic distance using an algorithm called GloVe (35), which has previously been used to score the Alternative Uses Task (26, 27). We used a freely available model that was pretrained on the Common Crawl corpus, which contains the text of billions of web pages (35).”",
        "page": 2,
        "note": ""
      },
      {
        "text": "“To provide some redundancy (as described below), we keep only the first seven valid words that participants provide. The DAT score is the transformed average of the semantic distances between these words. In particular, we compute the semantic distance (i.e., cosine distance) between all 21 possible pairs of the seven words, take the average, and then multiply it by 100.”",
        "page": 2,
        "note": ""
      },
      {
        "text": "“The minimum score (zero) occurs when there is no distance between the words: that is, when all of the words are the same. The theoretical maximum score (200) would occur when the words are as different from each other as possible. In practice, scores commonly range from 65 to 90 and almost never exceed 100. Scores under 50 are often due to misunderstanding the instructions, such as naming opposites (e.g., day and night) rather than unrelated words. In this way, the score can be intuitively thought of as a grade on an examination; under 50 is poor, the average is between 75 and 80, and 95 is a very high score.”",
        "page": 2,
        "note": ""
      },
      {
        "text": "“the DAT correlated with flexibility [r (55) = 0.51 [0.29, 0.68], P < 0.001] and originality [r (55) = 0.50 [0.28, 0.68], P < 0.001], but we did not see the same correlation with fluency [r (55) = 0.21 [−0.05, 0.45], P = 0.057]. Using the full dataset with no manual screening, we saw positive correlations across all three measures (flexibility: r = 0.34 [0.18, 0.48]; originality: r = 0.32 [0.16, 0.46]; fluency: r = 0.22 [0.06, 0.37]).”",
        "page": 2,
        "note": ""
      },
      {
        "text": "“Participants also completed the Bridge-the-Associative-Gap Task, a test of convergent thinking in which participants see two words (e.g., giraffe and scarf) and need to find a third one that relates to both (e.g., neck). Raters then judged the appropriateness of the provided words. We saw a positive correlation between the DAT and appropriateness in the manually screened subsample [r (54) = 0.34 [0.08, 0.55], P = 0.006] as well as in the full dataset [r (136) = 0.22 [0.06, 0.38], P = 0.004].”",
        "page": 2,
        "note": ""
      },
      {
        "text": "“Study 1B attempted to replicate these findings in another dataset, without any manual screening. We again saw positive correlations with the Alternative Uses Task [flexibility: r (223) = 0.35 [0.23, 0.46], P < 0.001; originality: r (223) = 0.32 [0.20, 0.43], P < 0.001; fluency: r (223) = 0.30 [0.17, 0.41], P < 0.001] and appropriateness ratings in the Bridge-theAssociative-Gap Task [r (203) = 0.23 [0.10, 0.36], P < 0.001].”",
        "page": 2,
        "note": ""
      },
      {
        "text": "“To assess test–retest reliability, in Study 1C participants completed the DAT during laboratory visits 2 wk apart for an unrelated study (38). Test–retest reliability was high [r (48) = 0.73 [0.57, 0.84], P < 0.001]; this reliability resembled that of completing the same Alternative Uses Task items 1 mo later, as scored by raters (r = 0.61 to 0.70) or an algorithm (r = 0.49 to 0.80) (39).”",
        "page": 2,
        "note": ""
      },
      {
        "text": "“results suggest that simply asking participants to name unrelated words can serve as a reliable measure of divergent thinking. We compared performance on this task with established creativity measures—the Alternative Uses Task and the Bridgethe-Associative-Gap Task—as well as related measures of insight and analytical problem solving. The correlations between the DAT and these measures tended to be at least as high as the correlations among the other established measures themselves, demonstrating strong convergent validity. The highest correlations were between the DAT and the Alternative Uses Task”",
        "page": 3,
        "note": ""
      },
      {
        "text": "“Test–retest reliability was also high over a span of 2 wk (Study 1C). Overall, the evidence supports semantic distance as a reliable measure of divergent thinking. Although the precise mechanism underlying this link is unclear, the DAT may indirectly measure the extent or efficiency of the association network, as suggested by associative (3, 4, 6) and executive control theories (2, 20).”",
        "page": 3,
        "note": ""
      },
      {
        "text": "“The DAT resolves a number of limitations in the creativity literature. Compared with manually scored tasks, the DAT scoring is automatic and objective, allowing researchers to collect large samples with little effort and no rater bias. Further, the scoring is absolute and not sample dependent, making it conducive to comparing diverse populations. The DAT may also reduce some biases seen in other tests of divergent thinking. Experiential bias occurs when one’s past experience influences the diversity of responses (45). When listing uses of a brick in the Alternative Uses Task, for example, a brick layer may provide different responses than a lawyer, leading to more uncontrollable variation. Similarly, different object prompts in the Alternative Uses Task can lead to different responses with varying reliability between computational scoring and manually scored responses (27). The DAT avoids these issues by giving an open-ended prompt and using a model trained on an international corpus (i.e., global website data). Relatedly, fluency contamination occurs when high fluency can artificially inflate originality scores; the more responses that participants list, the more likely some of them will be unique (22). The DAT avoids this issue by requiring all participants to generate the same number of responses.”",
        "page": 3,
        "note": ""
      },
      {
        "text": "“The DAT has several limitations. Naming unrelated words measures originality with better face validity than appropriateness, although both are important components of creativity (31). DAT scores may thus partly reflect other constructs more related to divergence than creativity, such as overinclusive thinking or schizotypy (46). Still, the DAT scores correlated with assessments of appropriateness on the Bridge-the-AssociativeGap Task. Another limitation is that participants may artificially modulate their scores using different strategies to generate the words. Intentionally choosing rare words, looking around the room for inspiration, and following letter-based strategies (e.g., choosing rhyming words) can all influence the overall score. The fairly short time limit of 4 min may reduce the likelihood that participants will consider and implement these various strategies. Beyond the task itself, a limitation of our study is the small number of measures used. Given the high correlation between originality and flexibility in the Alternative Uses Task scores and the low number of items in the problem-solving tasks, future studies could replicate our results using other scoring methods as well as longer and more diverse creativity measures.”",
        "page": 4,
        "note": ""
      }
    ]
  },
  {
    "key": "FoltzEtAl2023",
    "title": "Reflections on the nature of measurement in language-based automated assessments of patients' mental state and cognitive function",
    "authors": "Foltz et al.",
    "year": "2023",
    "publication": "Schizophrenia Research",
    "doi": "10.1016/j.schres.2022.07.011",
    "url": "https://www.sciencedirect.com/science/article/pii/S0920996422002833",
    "abstract": "Modern advances in computational language processing methods have enabled new approaches to the measurement of mental processes. However, the field has primarily focused on model accuracy in predicting performance on a task or a diagnostic category. Instead the field should be more focused on determining which computational analyses align best with the targeted neurocognitive/psychological functions that we want to assess. In this paper we reflect on two decades of experience with the application of language-based assessment to patients' mental state and cognitive function by addressing the questions of what we are measuring, how it should be measured and why we are measuring the phenomena. We address the questions by advocating for a principled framework for aligning computational models to the constructs being assessed and the tasks being used, as well as defining how those constructs relate to patient clinical states. We further examine the assumptions that go into the computational models and the effects that model design decisions may have on the accuracy, bias and generalizability of models for assessing clinical states. Finally, we describe how this principled approach can further the goal of transitioning language-based computational assessments to part of clinical practice while gaining the trust of critical stakeholders.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  },
  {
    "key": "HillEtAl2015",
    "title": "SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation",
    "authors": "Hill, Reichart & Korhonen",
    "year": "2015",
    "publication": "Computational Linguistics",
    "doi": "10.1162/COLI_a_00237",
    "url": "https://direct.mit.edu/coli/article/41/4/665-695/1517",
    "abstract": "We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.",
    "status": "cite",
    "role": {
      "establishes": [
        "H1.7: SimLex-999 benchmark for semantic similarity",
        "Distinction between similarity and relatedness"
      ],
      "supports_instruments": [
        "INS-001.1",
        "INS-001.2"
      ],
      "limitations": [
        "Evaluates population average, not individual differences"
      ]
    },
    "annotations": [
      {
        "text": "“The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977). At its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents.”",
        "page": 666,
        "note": ""
      },
      {
        "text": "“we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity. SimLex-999 was produced by 500 paid native English speakers, recruited via Amazon Mechanical Turk,2 who were asked to rate the similarity, as opposed to association, of concepts via a simple visual interface. The choice of evaluation pairs in SimLex-999 was motivated by empirical evidence that humans represent concepts of distinct part-of-speech (POS) (Gentner 1978) and conceptual concreteness (Hill, Korhonen, and Bentz 2014) differently. Whereas existing gold standards contain only concrete noun concepts (MEN) or cover only some of these distinctions via a random selection of items (WS-353, RG), SimLex-999 contains a principled selection of adjective, verb, and noun concept pairs covering the full concreteness spectrum. This design enables more nuanced analyses of how computational models overcome the distinct challenges of representing concepts of these types.”",
        "page": 667,
        "note": ""
      }
    ]
  },
  {
    "key": "RoarkEtAl2011",
    "title": "Spoken Language Derived Measures for Detecting Mild Cognitive Impairment",
    "authors": "Roark et al.",
    "year": "2011",
    "publication": "IEEE transactions on audio, speech, and language processing",
    "doi": "10.1109/TASL.2011.2112351",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3244269/",
    "abstract": "Spoken responses produced by subjects during neuropsychological exams can provide diagnostic markers beyond exam performance. In particular, characteristics of the spoken language itself can discriminate between subject groups. We present results on the utility of such markers in discriminating between healthy elderly subjects and subjects with mild cognitive impairment (MCI). Given the audio and transcript of a spoken narrative recall task, a range of markers are automatically derived. These markers include speech features such as pause frequency and duration, and many linguistic complexity measures. We examine measures calculated from manually annotated time alignments (of the transcript with the audio) and syntactic parse trees, as well as the same measures calculated from automatic (forced) time alignments and automatic parses. We show statistically significant differences between clinical subject groups for a number of measures. These differences are largely preserved with automation. We then present classification results, and demonstrate a statistically significant improvement in the area under the ROC curve (AUC) when using automatic spoken language derived features in addition to the neuropsychological test scores. Our results indicate that using multiple, complementary measures can aid in automatic detection of MCI.",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  }
]