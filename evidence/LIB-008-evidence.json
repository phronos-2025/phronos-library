[
  {
    "key": "CazaletsDambre2025",
    "title": "Word Synchronization Challenge: A Benchmark for Word Association Responses for LLMs",
    "authors": "Cazalets & Dambre",
    "year": "2025",
    "publication": "",
    "doi": "10.1007/978-3-031-93864-1_1",
    "url": "http://arxiv.org/abs/2502.08312",
    "abstract": "This paper introduces the Word Synchronization Challenge, a novel benchmark to evaluate large language models (LLMs) in Human-Computer Interaction (HCI). This benchmark uses a dynamic game-like framework to test LLMs ability to mimic human cognitive processes through word associations. By simulating complex human interactions, it assesses how LLMs interpret and align with human thought patterns during conversational exchanges, which are essential for effective social partnerships in HCI. Initial findings highlight the influence of model sophistication on performance, offering insights into the models capabilities to engage in meaningful social interactions and adapt behaviors in human-like ways. This research advances the understanding of LLMs potential to replicate or diverge from human cognitive functions, paving the way for more nuanced and empathetic human-machine collaborations.",
    "status": "cite",
    "annotations": [
      {
        "text": "“Assessing the effectiveness of LLMs goes beyond just measuring their linguistic accuracy. It involves evaluating how well they capture word associations and reflect the cognitive processes humans use in communication [13]. By analyzing these models’ ability to mimic human thought and emotional understanding, researchers can ensure that LLMs can engage in meaningful conversations”",
        "page": 1,
        "note": ""
      },
      {
        "text": "“Word associations, a fundamental aspect of both linguistic research and psychological studies [3], illuminate how concepts are interconnected within human cognition. They provide essential insights that can be used to program chatbots that mimic human thought processes and emotional responses. This capability is vital for chatbots to engage in meaningful dialogues and to act as social partners, adapting their behaviors in ways that are perceived as natural and considerate by human users.”",
        "page": 2,
        "note": ""
      },
      {
        "text": "“Since LLMs learn from datasets that may contain biases, their associative outputs can inadvertently perpetuate stereotypes or reflect cultural biases. By closely examining these associations, researchers can identify and mitigate unwanted biases in AI models, ensuring that these technologies act in ways that are fair, unbiased, and aligned with societal values. The Word Synchronization Challenge not only serves as a benchmark for technological advancement but also provides a foundation for a myriad of related tasks, including: – Social Interaction Evaluation: The primary goal of this benchmark is to assess the LLMs’ capacity to predict and adapt within social interactions, using their response to word associations to gauge their ability to mirror human reasoning and adapt their behaviors accordingly. These interactions are not merely linguistic tasks but are integral to developing chatbots capable of nuanced social participation and empathy. – Model Understanding: It helps us understand how LLMs organize and relate information. By examining word associations, researchers can gain insights into the underlying mechanisms of these models, such as how they capture semantic relationships or encode context. [15] [18] – Comparison with Human Cognition: Studying word associations in LLMs allows for comparisons with human cognitive processes. This can inform whether LLMs mimic human-like associative patterns or if they develop distinct mechanisms for linking concepts. [1] [16]”",
        "page": 3,
        "note": ""
      },
      {
        "text": "“The task entails a dyadic interaction where two participants, which can include the LLM, are involved in a repeated word production game. Goal : The primary objective of the game is for both participants to articulate the same word in a round. Rules : – Word Production • Each participant must produce a word. • Words produced in previous rounds cannot be used again. • In the first round, participants can choose any random word. – Evaluation • After both participants have spoken their words, the words are compared. • If both participants say the same word, the game ends and they have achieved the goal. • If the words are different, the game proceeds to the next step. – Reflection • Participants engage in a preparatory phase for the upcoming round. • During this phase, each participant reflects on the words spoken in the previous round and tries to understand the thought pattern of the other participant. This understanding will guide their word choice for the next round.”",
        "page": 4,
        "note": ""
      },
      {
        "text": "“Model Interaction and Prompting. Each game begins with a system message explaining the rules to both models. At every round, each model receives a prompt that includes: – A reminder that the goal is to produce the same word. – A list of previously used words (which cannot be repeated). – The last word provided by the opponent. The model then generates a single-word response, ensuring that it does not exceed 20 tokens. To encourage diverse word choices and prevent overly deterministic responses, we set the temperature parameter to 1.2. The game continues iteratively until either: – Both models generate the same word (win condition). – A model repeats a word from previous rounds (loss due to repetition). – A model produces a non-existent word (loss due to invalid input). – The game reaches the 20-round limit without convergence (loss due to nonconvergence).”",
        "page": 5,
        "note": ""
      },
      {
        "text": "“To ensure that models only use valid words, we query Wiktionary’s API to verify the existence of each word. If a word is not found, the game terminates with a loss due to invalid input. Dataset. The interaction history for each game is dynamically updated, preserving the sequence of exchanges. The system maintains: – A record of all words exchanged during the game. – The turn-by-turn progression of each model’s responses. – The final game outcome (win, loss due to repetition, loss due to invalid word, or loss due to non-convergence). – The models involved. This dataset serves as the foundation for further analysis, including semantic similarity assessments and strategy evaluations.”",
        "page": 6,
        "note": ""
      },
      {
        "text": "“Our examination involves calculating the Euclidean distances from each model’s word choices to two critical reference points: – The embedding of the previous word from the opposing model. – The average embedding of the two last words (one from each model). Models that consistently select words closer to the previous word of their opponent might be employing a mirroring strategy. This approach suggests a reactive behavior where the model potentially leverages the context set by the opponent’s last word to ensure coherence and continuity in word choice. This behavior might be indicative of models designed to enhance dialogue flow and maintain topic relevance. Alternatively, selecting words closer to the average embedding of the last two words indicates a balancing strategy. This method reflects a more independent and possibly creative approach to word selection, aiming to blend both participants’ inputs into a new direction that does not strictly follow the immediate cue but considers the broader context established by both models.”",
        "page": 8,
        "note": ""
      },
      {
        "text": "“Finally, the visualization of word embedding trajectories offers crucial insights into how two models adjust their language choices over time to align with each other’s cognitive models. By tracking these embeddings, we can observe the dynamic process of synchronization, which is fundamental to effective communication. In collaborative interactions, the convergence of word embeddings signals the achievement of a shared understanding, where both models coevolve toward the articulation of the same or semantically similar words.”",
        "page": 10,
        "note": ""
      },
      {
        "text": "“Notably, the initial utterances showed transitions between the first two manifolds (fruits and sky/water), revealing that the models employed a local mirroring strategy. By selecting words that were semantically close to the previous word from the other model, they attempted to bridge semantic gaps and maintain coherence in the conversation. For instance, the transition from Mango to Sky could be conceptually linked by color or as a juxtaposition of natural elements, which, while categorically distinct, share a relational context.”",
        "page": 11,
        "note": ""
      },
      {
        "text": "“In a remarkable instance of convergence, the two model outputs aligned showcasing an advanced degree of thematic and semantic synchronization. Both sequences share a deep focus on ecological and environmental themes, with the first placing a stronger emphasis on conservation, while the second delves into biological diversity. The semantic progression in each is meticulously structured, showcasing the models’ capabilities in maintaining thematic coherence. The convergence on the term Existence underlines a sophisticated conceptual synthesis, reflecting the models’ capacity for deep semantic integration. The analysis of the model outputs reveals the presence of distinct semantic manifolds, each representing a cluster of interrelated themes. These manifolds demonstrate the models’ capability to categorize and conceptualize complex information through semantic proximity.”",
        "page": 12,
        "note": ""
      },
      {
        "text": "“Beyond model integration, we aim to leverage the collected human interaction data to explore deeper cognitive mechanisms underlying word association and adaptation processes. The reasoning patterns behind user word choices serve as valuable signals for fine-tuning models, potentially improving their ability to mimic human reasoning and align more naturally with human communicative strategies. Additionally, the trajectory of word selections across multiple rounds offers insights into adaptive behaviors, shedding light on how users adjust their strategies over time in response to LLM interactions.”",
        "page": 16,
        "note": ""
      }
    ],
    "role": {
      "establishes": [
        "LLM-based word synchronization challenge design",
        "Semantic similarity game methodology"
      ],
      "supports_instruments": [
        "INS-001.2"
      ],
      "limitations": [
        "LLM-human collaboration context differs from pure assessment"
      ]
    }
  },
  {
    "key": "GoogleAISemantris",
    "title": "Semantris by Google AI - Experiments with Google",
    "authors": "Google AI",
    "year": "",
    "publication": "Google AI Experiments",
    "doi": "",
    "url": "https://experiments.withgoogle.com/semantris",
    "abstract": "Since 2009, coders have created thousands of amazing experiments using Chrome, Android, AI, WebVR, AR and more. We're showcasing projects here, along with helpful tools and resources, to inspire others to create new experiments.",
    "status": "cite",
    "annotations": []
  },
  {
    "key": "KumarEtAl2021",
    "title": "Semantic Memory Search and Retrieval in a Novel Cooperative Word Game: A Comparison of Associative and Distributional Semantic Models",
    "authors": "Kumar, Steyvers & Balota",
    "year": "2021",
    "publication": "Cognitive Science",
    "doi": "10.1111/cogs.13053",
    "url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13053",
    "abstract": "Considerable work during the past two decades has focused on modeling the structure of semantic memory, although the performance of these models in complex and unconstrained semantic tasks remains relatively understudied. We introduce a two-player cooperative word game, Connector (based on the boardgame Codenames), and investigate whether similarity metrics derived from two large databases of human free association norms, the University of South Florida norms and the Small World of Words norms, and two distributional semantic models based on large language corpora (word2vec and GloVe) predict performance in this game. Participant dyads were presented with 20-item word boards with word pairs of varying relatedness. The speaker received a word pair from the board (e.g., exam-algebra) and generated a one-word semantic clue (e.g., math), which was used by the guesser to identify the word pair on the board across three attempts. Response times to generate the clue, as well as accuracy and latencies for the guessed word pair, were strongly predicted by the cosine similarity between word pairs and clues in random walk-based associative models, and to a lesser degree by the distributional models, suggesting that conceptual representations activated during free association were better able to capture search and retrieval processes in the game. Further, the speaker adjusted subsequent clues based on the first attempt by the guesser, who in turn benefited from the adjustment in clues, suggesting a cooperative influence in the game that was effectively captured by both associative and distributional models. These results indicate that both associative and distributional models can capture relatively unconstrained search processes in a cooperative game setting, and Connector is particularly suited to examine communication and semantic search processes.",
    "status": "cite",
    "annotations": [],
    "role": {
      "establishes": [
        "Connector word game methodology",
        "Semantic search as game mechanic"
      ],
      "supports_instruments": [
        "INS-001.2"
      ],
      "limitations": [
        "Game design focus, less psychometric validation"
      ]
    }
  },
  {
    "key": "LumsdenEtAl2016",
    "title": "Gamification of Cognitive Assessment and Cognitive Training: A Systematic Review of Applications and Efficacy",
    "authors": "Lumsden et al.",
    "year": "2016",
    "publication": "JMIR Serious Games",
    "doi": "10.2196/games.5888",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC4967181/",
    "abstract": "Background\nCognitive tasks are typically viewed as effortful, frustrating, and repetitive, which often leads to participant disengagement. This, in turn, may negatively impact data quality and/or reduce intervention effects. However, gamification may provide a possible solution. If game design features can be incorporated into cognitive tasks without undermining their scientific value, then data quality, intervention effects, and participant engagement may be improved.\n\nObjectives\nThis systematic review aims to explore and evaluate the ways in which gamification has already been used for cognitive training and assessment purposes. We hope to answer 3 questions: (1) Why have researchers opted to use gamification? (2) What domains has gamification been applied in? (3) How successful has gamification been in cognitive research thus far?\n\nMethods\nWe systematically searched several Web-based databases, searching the titles, abstracts, and keywords of database entries using the search strategy (gamif* OR game OR games) AND (cognit* OR engag* OR behavi* OR health* OR attention OR motiv*). Searches included papers published in English between January 2007 and October 2015.\n\nResults\nOur review identified 33 relevant studies, covering 31 gamified cognitive tasks used across a range of disorders and cognitive domains. We identified 7 reasons for researchers opting to gamify their cognitive training and testing. We found that working memory and general executive functions were common targets for both gamified assessment and training. Gamified tests were typically validated successfully, although mixed-domain measurement was a problem. Gamified training appears to be highly engaging and does boost participant motivation, but mixed effects of gamification on task performance were reported.\n\nConclusions\nHeterogeneous study designs and typically small sample sizes highlight the need for further research in both gamified training and testing. Nevertheless, careful application of gamification can provide a way to develop engaging and yet scientifically valid cognitive assessments, and it is likely worthwhile to continue to develop gamified cognitive tasks in the future.",
    "status": "cite",
    "annotations": [],
    "role": {
      "establishes": [
        "Gamification efficacy for cognitive assessment",
        "Working memory common target for gamified assessment",
        "Engagement benefits of gamification"
      ],
      "supports_instruments": [
        "INS-001.1",
        "INS-001.2"
      ],
      "limitations": [
        "Heterogeneous study designs",
        "Small sample sizes in reviewed studies"
      ]
    }
  },
  {
    "key": "McNabDolan2014",
    "title": "Dissociating distractor-filtering at encoding and during maintenance",
    "authors": "McNab & Dolan",
    "year": "2014",
    "publication": "Journal of Experimental Psychology: Human Perception and Performance",
    "doi": "10.1037/a0036013",
    "url": "",
    "abstract": "The effectiveness of distractor-filtering is a potentially important determinant of working memory capacity (WMC). However, a distinction between the contributions of distractor-filtering at WM encoding as opposed to filtering during maintenance has not been made and the assumption is that these rely on the same mechanism. Within 2 experiments, 1 conducted in the laboratory with 21 participants, and the other played as a game on smartphones (n = 3,247) we measure WMC without distractors, and present distractors during encoding or during the delay period of a WM task to determine performance associated with distraction at encoding and during maintenance. Despite differences in experimental setting and paradigm design between the 2 studies, we show a unique contribution to WMC from both encoding and delay distractor performance in both experiments, while controlling for performance in the absence of distraction. Thus, within 2 separate experiments, 1 involving an extremely large cohort of 3,247 participants, we show a dissociation between encoding and delay distractor-filtering, indicating that separate mechanisms may contribute to WMC. (PsycINFO Database Record (c) 2016 APA, all rights reserved)",
    "status": "cite",
    "annotations": []
  },
  {
    "key": "McNabEtAl2015",
    "title": "Age-related changes in working memory and the ability to ignore distraction",
    "authors": "McNab et al.",
    "year": "2015",
    "publication": "Proceedings of the National Academy of Sciences",
    "doi": "10.1073/pnas.1504162112",
    "url": "https://www.pnas.org/doi/10.1073/pnas.1504162112",
    "abstract": "",
    "status": "cite",
    "annotations": []
  },
  {
    "key": "PedersenEtAl2023",
    "title": "Measuring Cognitive Abilities in the Wild: Validating a Population-Scale Game-Based Cognitive Assessment",
    "authors": "Pedersen et al.",
    "year": "2023",
    "publication": "Cognitive Science",
    "doi": "10.1111/cogs.13308",
    "url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13308",
    "abstract": "Rapid individual cognitive phenotyping holds the potential to revolutionize domains as wide-ranging as personalized learning, employment practices, and precision psychiatry. Going beyond limitations imposed by traditional lab-based experiments, new efforts have been underway toward greater ecological validity and participant diversity to capture the full range of individual differences in cognitive abilities and behaviors across the general population. Building on this, we developed Skill Lab, a novel game-based tool that simultaneously assesses a broad suite of cognitive abilities while providing an engaging narrative. Skill Lab consists of six mini-games as well as 14 established cognitive ability tasks. Using a popular citizen science platform (N = 10,725), we conducted a comprehensive validation in the wild of a game-based cognitive assessment suite. Based on the game and validation task data, we constructed reliable models to simultaneously predict eight cognitive abilities based on the users’ in-game behavior. Follow-up validation tests revealed that the models can discriminate nuances contained within each separate cognitive ability as well as capture a shared main factor of generalized cognitive ability. Our game-based measures are five times faster to complete than the equivalent task-based measures and replicate previous findings on the decline of certain cognitive abilities with age in our large cross-sectional population sample (N = 6369). Taken together, our results demonstrate the feasibility of rapid in-the-wild systematic assessment of cognitive abilities as a promising first step toward population-scale benchmarking and individualized mental health diagnostics.",
    "status": "anchor",
    "annotations": [
      {
        "text": "“Individual cognitive phenotyping holds the potential to revolutionize domains as wideranging as personalized learning, employment practices, and precision psychiatry. To get there, it will require us to rethink how we study and measure cognitive abilities. Much of what cognitive and behavioral scientists know about cognitive abilities and psychological behavior has been gleaned from studying small, homogeneous groups in the laboratory. Recent pushes to increase the number and diversity of participants (Bauer, 2020) are revolutionizing standards for power and generalizability across the cognitive and behavioral sciences. These advances have been enabled in part by moving from in-person testing to online equivalents, which are less costly for experimenters and more convenient for participants (Birnbaum, 2004).”",
        "page": 2,
        "note": ""
      },
      {
        "text": "“Going online with more convenient digital versions of traditional tasks makes it possible to scale up participant recruitment via crowdsourcing. Examples include projects, such as LabintheWild (Reinecke & Gajos, 2015), Volunteer Science (Radford et al., 2016), and TestMyBrain (Germine et al., 2012), which offer a broad suite of digitized tasks from cognitive and behavioral science to volunteers from the general public.”",
        "page": 2,
        "note": ""
      },
      {
        "text": "“new games are designed through an evidence-centered design process, whereby assessment tasks are designed to evoke behaviors that reveal targeted competencies (Mislevy, Almond, & Lukas, 2003). By designing a complete game from scratch around specific cognitive abilities, researchers can obtain richer information than the traditional pen and paper version (Hagler, Jimison, & Pavel, 2014). The games can be more complex and dynamic, which allows for more interesting cognitive modeling (Leduc-McNiven, White, Zheng, D McLeod, & R Friesen, 2018). Moreover, cognitive assessment games often apply stealth assessment (Shute, Wang, Greiff, Zhao, & Moore, 2016), where the cognitive ability measures are derived from the players’ in-game behavior.”",
        "page": 3,
        "note": ""
      },
      {
        "text": "“Prominent examples of games built for cognitive assessment and applied at a large scale are Sea Hero Quest (Coughlan et al., 2019) and The Great Brain Experiment (H. R. Brown et al., 2014). Sea Hero Quest delivers a casual game experience and has reached 2.5 million participants, which yielded important insights into spatial navigation impairments in adults at risk of Alzheimer’s disease (Coutrot et al., 2018). That said, Sea Hero Quest is by design only intended to measure spatial navigation; thus, if the goal is to measure a portfolio of distinct cognitive abilities, it would be a considerable effort to perform similar studies for each cognitive ability of interest. In contrast, The Great Brain Experiment is a collection of smaller games that assess multiple cognitive abilities. Through a large-scale deployment, the games have yielded new insights into age-related changes in working memory performance (McNab et al., 2015) and patterns of bias in information-seeking behavior (Hunt, Rutledge, Malalasekera, Kennerley, & Dolan, 2016).”",
        "page": 3,
        "note": ""
      },
      {
        "text": "“This, thus, raises an important question: How can robust within-subject validation of game-based cognitive ability measures be achieved by motivating large groups of players to both play the games as well as perform the less entertaining and more time-consuming traditional cognitive tasks?”",
        "page": 4,
        "note": ""
      },
      {
        "text": "“Here, we present Skill Lab, an original suite of games that takes advantage of the demonstrated power of online recruitment to validate novel gamified assessments of a broad portfolio of cognitive abilities. Our comprehensive mapping of multiple abilities within the same game allows us to assess their interrelations, as well as correlations with participant demographic factors, in a broad cross-section of a national population.”",
        "page": 4,
        "note": ""
      },
      {
        "text": "“Central executive functioning has several definitions. It is proposed to include various cognitive functions, such as planning, inhibiting responses, developing strategies, flexible action sequencing, and maintaining behavior. Essentially, central executive functioning consists of various classes of behavior used in self-regulation (Logan, 1985). Therefore, an executive act is any action toward oneself (whether conscious or not) that functions to change one’s behavior to change future outcomes (Barkley, 2001).”",
        "page": 5,
        "note": ""
      },
      {
        "text": "“Written language comprehension is the ability to process textual information. At the sentence level, processing involves many subcomponents, such as recognizing individual written words, understanding how the words relate to each other, how the words fit together in sentences, and how the context constrains the interpretation of the sentence (Rodd, Vitello, Woollams, & Adank, 2015).”",
        "page": 5,
        "note": ""
      },
      {
        "text": "“Visual processing is the ability to perceive, process, analyze, and manipulate visual information and involves the storage and recall of visual representations via visual imagery and memory (Castro-Alonso & Atit, 2019).”",
        "page": 5,
        "note": ""
      },
      {
        "text": "“Visual working memory involves storing and maintaining visual information in the short term (L. A. Brown, Forbes, & McConnell, 2006).”",
        "page": 5,
        "note": ""
      },
      {
        "text": "“Simple reaction time refers to the time needed to respond to a single stimulus as quickly as possible. Performance on simple reaction time tasks very often correlates with the performance of other psychometric tests. It is believed to indicate cognitive processing speed and is one of the most basic measurements of cognitive performance, underlying all cognitive functions. Studies of reaction times are critical in studies about aging, as reaction times increase with age (Deary, Liewald, & Nissan, 2011)”",
        "page": 5,
        "note": ""
      },
      {
        "text": "“Choice reaction time involves making appropriate responses as quickly as possible when challenged with two or more response options. Choice reaction time captures aspects of processing speed under complex task conditions and shows a moderate to strong correlation with general fluid intelligence (Deary et al., 2011)”",
        "page": 5,
        "note": ""
      },
      {
        "text": "“Response inhibition is the ability to stop oneself from performing an action when the action is no longer required or is inappropriate. Inhibiting one’s responses is a component of executive functioning, as it supports flexible and goal-oriented behavior in changing contexts (Verbruggen & Logan, 2008).”",
        "page": 5,
        "note": ""
      },
      {
        "text": "“Cognitive flexibility refers to shifting between different tasks depending on contextual demands and is a component of executive function. Cognitive flexibility is vital in life, as we are faced with situations that require multitasking or rapid task switching every day. When talking about cognitive flexibility, the concept of switching costs is fundamental. Studies show that switching back and forth between tasks can harm productivity. On the other hand, task switching can be beneficial when stuck, as it can increase creativity by decreasing cognitive fixation (Geurts, Corbett, & Solomon, 2009; Lu, Akinola, & Mason, 2017; Monsell, 2003)”",
        "page": 6,
        "note": ""
      },
      {
        "text": "“Planning refers to the ability to anticipate and plan actions, as well as to monitor goal attainment, and when necessary, update plans mid-execution. It involves a supervising function that is linked with the frontal activation and is essential for successful problem-solving (Dockery, Hueckel-Weng, Birbaumer, & Plewnia, 2009). Planning is often viewed as a subcomponent of executive functioning (Carlson, Moses, & Claxton, 2004; Krikorian, Bartok, & Gay, 1994)”",
        "page": 6,
        "note": ""
      },
      {
        "text": "“Color perception is the ability to detect differences in stimuli with varying distributions of spectral energy. These differences must be based on the color’s hue or saturation rather than the intensity contrast of the stimuli (Jacobs, 1993).”",
        "page": 6,
        "note": ""
      },
      {
        "text": "“Categorical visual perception refers to the ability to organize concepts (e.g., objects or attributes of objects) into distinct categories, with the consequence that cross-category stimuli will be more easily distinguishable than within-category stimuli (Harnad, 1987).”",
        "page": 6,
        "note": ""
      },
      {
        "text": "“Recognition is the ability to identify information–in this case about objects–from previous encounters or knowledge, such as shape or color. This is a cue-based, associative process and is related to visual search processes (Ullman, 2000).”",
        "page": 6,
        "note": ""
      },
      {
        "text": "“Mental rotation is the ability to mentally rotate objects and scenes and recognize them when looking at them from various orientations. This skill is closely related to navigational skills (Collins & Kimura, 1997) and several discrete processes involved during visual search, transformation, and recognition (Xue et al., 2017).”",
        "page": 6,
        "note": ""
      },
      {
        "text": "“Participant engagement typically has an exponential fall-off (Lieberoth, Pedersen, Marin, & Sherson, 2014), and in this case, a substantial player effort was needed to play both the games and complete the validation tasks; thus, broad and efficient recruitment was essential. Skill Lab was, therefore, launched publicly in Denmark in collaboration with the Public Danish Broadcast Company (Danmarks Radio, DR) on the 4th of September 2018 on (https: //www.scienceathome.org/games/skill-lab-science-detective/, Retrieved: 2020-07-07), Apple Appstore, and Google Play. The Committee of Research Ethics for Region Midtjylland (Denmark) exempted the study from ethical oversight, and the project received ethical approval from the Institutional Review Board at Cornell University (Protocol ID: 1808008201). The study was conducted in accordance with all ethical requirements. Thus, the players provided informed consent before taking part in the study and any data were recorded. The players were made aware that they could, at any time, leave the study and request their data to be anonymized.”",
        "page": 8,
        "note": ""
      },
      {
        "text": "“For all eight cognitive models, we find that 23–63% of the correlation is not due to the main factor, demonstrating the discriminative validity of the models. In other words, we clearly document that each of our models tap significantly into aspects beyond just the general abilities factor.”",
        "page": 18,
        "note": ""
      },
      {
        "text": "“The average time taken to complete all six games was 14 min (SD = 5 min), in comparison with 72 min needed to complete all the validation tasks (SD = 7). In other words, the Skill Lab games could model cognitive abilities in one-fifth of the time as required by the traditional set of cognitive tests.”",
        "page": 18,
        "note": ""
      },
      {
        "text": "“It should be mentioned that there was no nudging toward the tasks within Skill Lab and no requirement to do so; thus, there were no expectations from a data collection perspective toward the in-the-wild players completing all tasks. In particular, it should be noted that such a large fraction of the players identified sufficiently with the scientific purpose of the games (to help the researchers better understand human cognition) that they spent so much time performing the rather tedious validation tasks without any form of extrinsic reward.”",
        "page": 19,
        "note": ""
      },
      {
        "text": "“our current study is limited in that people were only recruited to play the game once. In order to be considered as a potential clinical tool in one-off as well as longitudinal applications, a follow-up test-retest study is needed to assess the robustness of our cognitive ability estimates. In such a test-retest setup, we could control the time between playthroughs to neutralize learning effects and ensure all the games have been played in both playthroughs. It is not unreasonable to expect that we could achieve even more consistent estimates by training models dependent on the playthrough number, compensating for learning effects due to the player familiarizing themselves with the tasks. Another consideration regarding reliability is the fact that our validation population set exhibited a slightly different gender distribution compared to the overall data set (64% vs. 49% females overall”",
        "page": 20,
        "note": ""
      },
      {
        "text": "“An alternative to the computational approach we present in this paper of aggregating indicators from multiple tasks is testing the feasibility of predicting individual task indicators from game data, which is more in line with the conventional literature (Salthouse, 2011). However, predicting individual indicators is not very robust, so we made the pragmatic choice of defining aggregated cognitive abilities measures (Bollen & Bauldry, 2011) while only combining task indicators associated with a cognitive ability in the theory to strengthen its interpretation.”",
        "page": 21,
        "note": ""
      },
      {
        "text": "“The raw and processed data that support the findings of this study are available together with the data processing scripts on the Open Science Framework”",
        "page": 22,
        "note": ""
      }
    ],
    "role": {
      "establishes": [
        "H8.1: Game-based cognitive assessment validity",
        "Skill Lab 6 mini-games simultaneously assess 8 cognitive abilities",
        "5x faster than task-based measures",
        "Cross-sectional age decline replication"
      ],
      "supports_instruments": [
        "INS-001.1",
        "INS-001.2",
        "INS-002"
      ],
      "limitations": [
        "N=10,725 citizen science sample",
        "Cross-sectional not longitudinal"
      ]
    }
  },
  {
    "key": "PinelliHiggins2025",
    "title": "The impact of co-creation on life's meaningfulness",
    "authors": "Pinelli & Higgins",
    "year": "2025",
    "publication": "Social Psychology",
    "doi": "10.1027/1864-9335/a000572",
    "url": "",
    "abstract": "Despite shared reality's ubiquity and importance in life, how the process of reaching it affects life’s meaning has not been explored yet. In three studies, we examine co-creation versus simple validation of opinions in enhancing life's meaning, controlling for the experience of inner states’ commonality. Study 1 shows that more co-creation correlates with greater life meaning beyond shared reality and personality traits. Study 2, a daily diary study, reveals that daily co-creation is positively associated with life's meaning via self-efficacy. Study 3 demonstrates that recalling co-creation events provides more meaning than mere agreement, an effect self-efficacy mediates. (PsycInfo Database Record (c) 2025 APA, all rights reserved)",
    "status": "cite",
    "annotations": []
  },
  {
    "key": "RaffertyEtAl2014",
    "title": "Optimally designing games for behavioural research",
    "authors": "Rafferty, Zaharia & Griffiths",
    "year": "2014",
    "publication": "Proceedings. Mathematical, Physical, and Engineering Sciences / The Royal Society",
    "doi": "10.1098/rspa.2013.0828",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC4032552/",
    "abstract": "Computer games can be motivating and engaging experiences that facilitate learning, leading to their increasing use in education and behavioural experiments. For these applications, it is often important to make inferences about the knowledge and cognitive processes of players based on their behaviour. However, designing games that provide useful behavioural data are a difficult task that typically requires significant trial and error. We address this issue by creating a new formal framework that extends optimal experiment design, used in statistics, to apply to game design. In this framework, we use Markov decision processes to model players' actions within a game, and then make inferences about the parameters of a cognitive model from these actions. Using a variety of concept learning games, we show that in practice, this method can predict which games will result in better estimates of the parameters of interest. The best games require only half as many players to attain the same level of precision.",
    "status": "cite",
    "annotations": []
  },
  {
    "key": "RossignacMilonEtAl2024",
    "title": "The role of target-specific shared reality in interpersonal interactions and protective health behaviours",
    "authors": "Rossignac-Milon et al.",
    "year": "2024",
    "publication": "European Journal of Social Psychology",
    "doi": "10.1002/ejsp.3095",
    "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.3095",
    "abstract": "Shared reality—the experience of sharing common inner states (e.g. feelings, beliefs) with other people about a given topic or target—is a ubiquitous human experience. With research on the construct of shared reality burgeoning in various domains, we examined a fundamental, yet understudied topic: the role of experiencing shared reality about a target in real-time conversations and real-world contexts. Across five studies conducted in various contexts (total N = 973), we developed a measure of target-specific shared reality (SR-T) and examined its role in interpersonal interactions and protective health behaviours. In our initial Studies (1a-2), we developed a measure of SR-T and establish psychometric, construct and criterion validity. In Study 3, we established predictive validity by investigating the link between SR-T and important interpersonal interaction constructs (e.g. interpersonal rapport and epistemic trust in the partner). In Study 4 (preregistered), SR-T moderated the effect of close others’ attitudes on vaccination and precautionary behaviours against COVID-19 during the Omicron-variant peak (2022). Our findings suggest that the experience of SR-T, assessed with a valid measure, is linked to important dimensions of interpersonal interactions and health decisions in the real world.",
    "status": "cite",
    "annotations": []
  },
  {
    "key": "RossignacMilonEtAl2025",
    "title": "More real together: conversation predicts realness through shared reality",
    "authors": "Rossignac-Milon et al.",
    "year": "2025",
    "publication": "Self and Identity",
    "doi": "10.1080/15298868.2025.2597803",
    "url": "https://www.tandfonline.com/doi/full/10.1080/15298868.2025.2597803",
    "abstract": "How do people make sense of the world and determine what is real? We propose that conversation enhances the perceived realness of everyday objects and experiences by allowing conversation partners to develop a shared reality — the perception of sharing the same beliefs and feelings about the world. We found that conversation partners who spontaneously talked more about ambiguous images perceived them as more real, and this effect was mediated by their sense of shared reality (Study 1). A 5-day daily diary study revealed that on days participants talked with a close partner more than usual, their experiences felt more real, and this effect was mediated by shared reality (Study 2). Experimentally introducing doubt in romantic couples’ beliefs about their shared reality disrupted the effect of conversation on realness, with effects persisting one week later (Study 3). These studies suggest that conversations may heighten perceived realness by enhancing shared reality, highlighting the importance of interpersonal relationships in shaping people’s sense of reality.",
    "status": "cite",
    "annotations": []
  },
  {
    "key": "StephensonEtAl2024",
    "title": "Codenames as a Benchmark for Large Language Models",
    "authors": "Stephenson, Sidji & Ronval",
    "year": "2024",
    "publication": "",
    "doi": "10.48550/arXiv.2412.11373",
    "url": "http://arxiv.org/abs/2412.11373",
    "abstract": "In this paper, we propose the use of the popular word-based board game Codenames as a suitable benchmark for evaluating the reasoning capabilities of Large Language Models (LLMs). Codenames presents a highly interesting challenge for achieving successful AI performance, requiring both a sophisticated understanding of language, theory of mind, and epistemic reasoning capabilities. Prior attempts to develop agents for Codenames have largely relied on word embedding techniques, which have a limited vocabulary range and perform poorly when paired with differing approaches. LLMs have demonstrated enhanced reasoning and comprehension capabilities for language-based tasks, but can still suffer in lateral thinking challenges. We evaluate the capabilities of several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5 Sonnet, and Llama 3.1, across a variety of board setups. Our results indicate that while certain LLMs perform better than others overall, different models exhibit varying emergent behaviours during gameplay and excel at specific roles. We also evaluate the performance of different combinations of LLMs when playing cooperatively together, demonstrating that LLM agents are more generalisable to a wider range of teammates than prior techniques.",
    "status": "cite",
    "annotations": [],
    "role": {
      "establishes": [
        "Codenames as LLM benchmark",
        "Language understanding + theory of mind assessment"
      ],
      "supports_instruments": [
        "INS-001.2"
      ],
      "limitations": [
        "LLM evaluation focus, not human cognitive assessment"
      ]
    }
  },
  {
    "key": "XuEtAl2025",
    "title": "Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs",
    "authors": "Xu, Deng, Zhou & Zhong",
    "year": "2025",
    "publication": "",
    "doi": "10.48550/arXiv.2505.17512",
    "url": "http://arxiv.org/abs/2505.17512",
    "abstract": "Concepts represent generalized abstractions that enable humans to categorize and reason efficiently, yet it is unclear to what extent Large Language Models (LLMs) comprehend these semantic relationships. Existing benchmarks typically focus on factual recall and isolated tasks, failing to evaluate the ability of LLMs to understand conceptual boundaries. To address this gap, we introduce CK-Arena, a multi-agent interaction game built upon the Undercover game, designed to evaluate the capacity of LLMs to reason with concepts in interactive settings. CK-Arena challenges models to describe, differentiate, and infer conceptual boundaries based on partial information, encouraging models to explore commonalities and distinctions between closely related concepts. By simulating real-world interaction, CK-Arena provides a scalable and realistic benchmark for assessing conceptual reasoning in dynamic environments. Experimental results show that LLMs' understanding of conceptual knowledge varies significantly across different categories and is not strictly aligned with parameter size or general model capabilities. The data and code are available at the project homepage: https://ck-arena.site.",
    "status": "cite",
    "annotations": [
      {
        "text": "“Traditional benchmarks for LLMs evaluation have contributed to improvements in model performance [11, 12, 13, 14], but they exhibit significant limitations. These benchmarks primarily assess tokenlevel accuracy and factual recall through static question-answer formats, often breaking down knowledge into isolated questions. This fragmented evaluation approach captures surface-level information retrieval but fails to probe the inherent connections and boundaries between concepts. For example, a model may correctly identify that monkeys and apes belong to Primates, yet this does not indicate any understanding of the structural relationships or distinctive features that separate these groups within the broader taxonomy.”",
        "page": 1,
        "note": ""
      },
      {
        "text": "“A well-designed benchmark is crucial to provide a standardized approach for evaluating LLMs in concept-based tasks, allowing effective measurement, comparison, and improvement of these models in concept comprehension and knowledge application. Simultaneously, interactive game-based environments have gained traction as novel evaluation paradigms to overcome the static nature of traditional benchmarks [18, 19, 20]. Unlike static question-answer formats, game-based evaluations create richer contexts for multi-step reasoning and decision-making. However, most game simulations primarily assess strategic reasoning, offering limited insight into the internal knowledge of models and their ability to convey structured concepts in dynamic multi-agent interactions.”",
        "page": 2,
        "note": ""
      },
      {
        "text": "“we propose Conceptual Knowledge Arena (CK-Arena), a multi-agent interaction game benchmark inspired by Undercover [21].”",
        "page": 2,
        "note": ""
      },
      {
        "text": "“Commonsense reasoning benchmarks play an important role in assessing the capabilities of Large Language Models (LLMs). Widely used benchmarks such as Story Cloze Test [14], Choice of Plausible Alternatives (COPA) [22], and HellaSwag [12] largely rely on static formats like multiple-choice questions or binary judgments. While effective for evaluating factual recall and surface-level understanding, these static formats do not fully reflect real-world interactive scenarios. More recent benchmarks, including MMLU [11], CMMLU [23], BIG-Bench [24], and HELM [13], have introduced tasks such as logical reasoning, cloze tests, and multi-turn Q&A to expand the scope of evaluation. Although these efforts represent progress toward more interactive assessments, they still focus predominantly on factual recall and task-specific reasoning, offering limited insight into how well LLMs understand and manipulate conceptual knowledge boundaries in evolving contexts. In contrast, CK-Arena is designed to explicitly evaluate conceptual reasoning by immersing LLMs in interactive, multi-agent gameplay that requires real-time understanding of semantic boundaries.”",
        "page": 3,
        "note": ""
      },
      {
        "text": "“CK-Arena is built on the multi-agent language game Undercover [21], which is originally designed to test the players’ reasoning and strategic communication abilities.”",
        "page": 3,
        "note": ""
      },
      {
        "text": "“players are assigned either as “civilians\" who are the majority of the players and know a common word, or as “undercover\" who are given a different but related word. Note that each player is informed of their assigned concept word but remains unaware of their team identity or the concepts held by others. Through rounds of description, players must identify who the undercover agents are while undercover agents try to remain undetected by providing descriptions vague enough to seem plausible without revealing their ignorance of the civilians’ word. After each round, players participate in a voting process to eliminate the individual they suspect to be an undercover agent. The game concludes under one of two conditions: (1) if all undercover agents are eliminated, the civilians win; (2) if the number of civilians and undercover agents is equal, the undercover agents win.”",
        "page": 4,
        "note": ""
      },
      {
        "text": "“During the testing phase of the Undercover game in CK-Arena, we observed that certain LLMs with smaller parameter sizes or older architectures struggled with the reasoning and decision-making requirements necessary for effective participation. To accommodate these models, we introduced a variant game mode, called Undercover-Audience, designed to simplify the game’s cognitive demands while still evaluating conceptual understanding. In the Undercover-Audience, all players are directly informed of both concepts as well as their own identities (civilian or undercover agent). Rather than attempting to obscure their concept, players focus on describing the common features shared between the two concepts. This adjustment reduces the need for strategic reasoning, making the game accessible to models with more limited reasoning capabilities. To replace the traditional voting mechanism, we introduce an audience character. This audience agent is unaware of the two concepts and the identities of the players. After each round of descriptions, the audience agent selects the player whose statements appear most inconsistent or unsociable with the shared features. This modified setup still allows for effective evaluation of conceptual grasp, as successful players must articulate the overlapping characteristics convincingly while avoiding detection.”",
        "page": 4,
        "note": ""
      },
      {
        "text": "“To illustrate the effectiveness of the Undercover game used in CK-Arena, consider a concrete example: suppose the concepts football and basketball are assigned to the players, with basketball designated as the undercover concept. During the speaking phase, the undercover player must analyze the descriptions provided by others about football, identify shared attributes, and strategically describe basketball in a way that overlaps with common features, such as “This is a ball-shaped sports equipment” or “This sport is played by two teams.” This requires more than surface-level token predictions—it involves understanding conceptual commonalities and distinctions. If the player merely relies on token-based generation without grasping these relationships, they are more likely to expose their undercover role, leading to elimination. Thus, performance in CK-Arena reflects the model’s understanding of conceptual knowledge boundaries.”",
        "page": 4,
        "note": ""
      },
      {
        "text": "“CK-Arena involves multiple LLMs as judges and LLM-based players for evaluation, with adjustable group sizes based on the experimental setup. In our experiments, the configuration includes 2 LLM judges and 6 LLM players, consisting of 4 civilians and 2 undercover agents. The game begins with an initialization phase, where players are randomly assigned their roles. Civilians receive a primary concept, while undercover agents are given a similar but distinct concept. During gameplay, each player takes turns making statements that describe their assigned concept while also attempting to identify potential undercover agents or civilians. After each statement, the LLM judges evaluate the description across three criteria: novelty, relevance, and reasonableness. If a player’s statement score falls below a predefined threshold, that player is automatically eliminated. This process continues for a predetermined number of rounds, after which an audience vote determines one additional player for elimination. The game progresses until one of three conditions is met: (1) all undercover agents are eliminated, resulting in a civilian victory; (2) the number of undercover agents matches the number of civilians, resulting in an undercover victory; or (3) the maximum number of rounds is reached. To maintain fairness and mitigate biases that may arise from LLM-based evaluations, multiple LLM judges with different strong base models are employed. The system records the mean and variance of their ratings for each statement. If the variance exceeds a predefined threshold, human reviewers consult a knowledge base to verify the judgment and adjust the final score if necessary.”",
        "page": 4,
        "note": ""
      },
      {
        "text": "“Novelty: It measures the extent to which a statement introduces new information compared to previous descriptions in the game. High novelty scores indicate that the statement presents fresh insights or unique perspectives, while low scores suggest repetition or rephrasing of earlier descriptions. The purpose of this metric is to discourage simple repackaging of information and promote creative exploration of concept characteristics. Statements falling below the novelty threshold result in automatic elimination to maintain engagement and meaningful discourse. (2) Reasonableness: This metric assesses the logical coherence between the statement and the inherent properties of the assigned concept. High scores indicate that the statement logically matches the concept’s attributes, while low scores suggest inconsistent or arbitrary descriptions. Ensuring reasonableness prevents players from making misleading or nonsensical claims during gameplay. Statements that fall below the reasonableness threshold trigger immediate elimination to preserve the integrity of the game. (3) Relevance: it evaluates how closely a statement aligns with the target concept. High relevance scores reflect descriptions that are specific and closely tied to the concept, making it easier for civilians to identify undercover agents. Conversely, low relevance indicates vague or overly broad descriptions that could apply to multiple concepts. This metric captures the strategic tension in the game—while civilians benefit from clear and targeted descriptions, undercover agents may intentionally opt for broader statements to avoid detection. Although relevance does not directly measure quality, it serves as a valuable scoring criterion for deeper analysis. All three metrics are generated by the Judge agent and reviewed for consistency and accuracy by human evaluators, ensuring fair and meaningful assessment throughout the game.”",
        "page": 6,
        "note": ""
      }
    ]
  },
  {
    "key": "YinEtAl2024",
    "title": "AI can help people feel heard, but an AI label diminishes this impact",
    "authors": "Yin, Jia & Wakslak",
    "year": "2024",
    "publication": "Proceedings of the National Academy of Sciences",
    "doi": "10.1073/pnas.2319112121",
    "url": "https://pnas.org/doi/10.1073/pnas.2319112121",
    "abstract": "People want to “feel heard” to perceive that they are understood, validated, and valued. Can AI serve the deeply human function of making others feel heard? Our research addresses two fundamental issues: Can AI generate responses that make human recipients feel heard, and how do human recipients react when they believe the response comes from AI? We conducted an experiment and a follow-up study to disentangle the effects of actual source of a message and the presumed source. We found that AI-generated messages made recipients feel more heard than human-generated messages and that AI was better at detecting emotions. However, recipients felt less heard when they realized that a message came from AI (vs. human). Finally, in a follow-up study where the responses were rated by third-party raters, we found that compared with humans, AI demonstrated superior discipline in offering emotional support, a crucial element in making individuals feel heard, while avoiding excessive practical suggestions, which may be less effective in achieving this goal. Our research underscores the potential and limitations of AI in meeting human psychological needs. These findings suggest that while AI demonstrates enhanced capabilities to provide emotional support, the devaluation of AI responses poses a key challenge for effectively leveraging AI’s capabilities.",
    "status": "cite",
    "annotations": [],
    "role": {
      "establishes": [
        "AI vs human emotional support comparison",
        "Feeling heard phenomenon with AI"
      ],
      "supports_instruments": [],
      "limitations": [
        "Devaluation effect when AI source known"
      ]
    }
  },
  {
    "key": "Unknown",
    "title": "Age-related changes in working memory and the ability to ignore distraction",
    "authors": "Unknown",
    "year": "",
    "publication": "",
    "doi": "10.1073/pnas.1504162112",
    "url": "https://www.pnas.org/doi/10.1073/pnas.1504162112",
    "abstract": "",
    "status": "cite",
    "role": {
      "establishes": [],
      "supports_instruments": [],
      "limitations": []
    },
    "annotations": []
  }
]